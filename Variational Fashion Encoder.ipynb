{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Fashion Encoder\n",
    "\n",
    "In this experiment I want to try out [Variational Auto Encoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "on a new fashion classification dataset from [Zalando](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "First copy the /data/fashion/ folder from the zalando repository next to this notebook.\n",
    "\n",
    "I will build a generic Variational Auto Encoder and then learn it on the new fashion-mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from scipy.misc import imsave\n",
    "from scipy.misc import imresize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    0: \"T-shirt-top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets('data/fashion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = data.train.num_examples\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder\n",
    "\n",
    "![VAE](images/vae_small.png)\n",
    "\n",
    "+ The bottom part of the model is embedding the input X into a mean and variance vector\n",
    "+ The mean and variance represent the parameters of a gaussian that is trained to be close to a standard normal distribution N(0, I)\n",
    "+ The decoder network is trying to reconstruct the input from a sample from said distribution\n",
    "+ Implementation is inspired by hwalsuklee [github](https://github.com/hwalsuklee/tensorflow-mnist-VAE/blob/master/vae.py)\n",
    "\n",
    "First lets define some standard utils to make construction of the neural network easier. We define an\n",
    "initialization method called Xavier that samples uniformly: \n",
    "$$(-\\sqrt{\\frac{6}{in + out}}, \\sqrt{\\frac{6}{in + out}})$$\n",
    "We then define a layer by the weights(Xavier) and biases(zeros) and it's result as:\n",
    "$$z = x * W +b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier(nin, nout):\n",
    "    hi = np.sqrt( 6 / (nin + nout))\n",
    "    lo = -hi\n",
    "    w  = tf.random_uniform((nin, nout), minval=lo, maxval=hi, dtype= tf.float32)\n",
    "    return w\n",
    "\n",
    "def bias(nout):\n",
    "    return tf.zeros([nout], dtype=tf.float32)\n",
    "\n",
    "def layer(x, l, nin, nout):\n",
    "    w = tf.Variable(xavier(nin, nout), name=\"W\" + str(l))    \n",
    "    b = tf.Variable(bias(nout), name= \"b\" + str(l))\n",
    "    z = tf.matmul(x, w) + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the encoder part the input is fed through feed forward layers multiple layers. In this case I chose to\n",
    "use ReLu activations except for the output. The output layer is special since it's activation is used\n",
    "as the parameters of a multivariate normal disribution with a diagonal covariance matrix or in other words a variance vector.The mean vector and the variance vector are concatenated.\n",
    "$$output = [\\mu, \\sigma]$$\n",
    "That means that the encoders output needs to be twice as large.\n",
    "Furthermore, the output's activation is a tanh function. A ReLu function's\n",
    "output is between 0 and 1 but a normal distribution has real valued parameters.\n",
    "A tanh gives values between -1 and 1 which is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x, shapes):         \n",
    "    a = x\n",
    "    l = 0\n",
    "    for nin, nout in shapes:\n",
    "        if l == len(shapes) - 1:\n",
    "            z = layer(a, l, nin, nout * 2) # In the last layer, the embedding represents the mean and variance concat\n",
    "            a = tf.nn.tanh(z)\n",
    "        else:            \n",
    "            z = layer(a,l, nin, nout)\n",
    "            a = tf.nn.relu(z)\n",
    "        l += 1\n",
    "    n_out = int(int(a.shape[1]) / 2)       \n",
    "    mean = a[:, n_out:]\n",
    "    sigm = 1e-6 + tf.nn.softplus(a[:, :n_out])\n",
    "    return (mean, sigm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder network is a simple feed forward net with ReLu activations and\n",
    "a sigmoid output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(x, shapes):         \n",
    "    a = x\n",
    "    l = 0\n",
    "    for nin, nout in shapes:     \n",
    "        z = layer(a, l, nin, nout)\n",
    "        if l == 0:\n",
    "            a = tf.nn.tanh(z)\n",
    "        elif l == len(shapes) - 1:\n",
    "            a = tf.nn.sigmoid(z)\n",
    "        else:            \n",
    "            a = tf.nn.relu(z)\n",
    "        l += 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual variation autoencder then passes the input through the encoder\n",
    "receiving the mean and variance of the normal distribution. \n",
    "A sample is drawn from said distribution and passed into the decoder.\n",
    "\n",
    "The loss for the decoder is defined using the difference between the input and the output.\n",
    "The encoder loss uses the kullback leibler divergence to a standard normal.\n",
    "The ELBO is the expectaion lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae(x, enc_shapes, dec_shapes):\n",
    "    mu, sigm = encoder(x, enc_shapes)\n",
    "    sample = mu + sigm * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)\n",
    "    y = decoder(sample, dec_shapes)\n",
    "    \n",
    "    marginal_likelihood = tf.reduce_sum(x * tf.log(y) + (1 - x) * tf.log(1 - y), 1)\n",
    "    KL_divergence = 0.5 * tf.reduce_sum(tf.square(mu) + tf.square(sigm) - tf.log(1e-8 + tf.square(sigm)) - 1, 1)\n",
    "\n",
    "    marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "    KL_divergence = tf.reduce_mean(KL_divergence)\n",
    "\n",
    "    ELBO = marginal_likelihood - KL_divergence\n",
    "\n",
    "    loss = -ELBO\n",
    "    return y, sample, loss, -marginal_likelihood, KL_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is basically the encoder reversed. The learning is set up below using the\n",
    "adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(shapes):\n",
    "    x = [(o, i) for i, o in shapes]\n",
    "    x.reverse()\n",
    "    return x\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "enc_shapes = [\n",
    "    (784, 512),\n",
    "    (512, 256),\n",
    "    (256, 128)\n",
    "]\n",
    "dec_shapes = reverse(enc_shapes)\n",
    "\n",
    "rate = 0.001\n",
    "batch_size = 100\n",
    "total_batch = int(n_samples / batch_size)\n",
    "epochs = 15\n",
    "\n",
    "y, z, loss, neg_marginal_likelihood, KL_divergence = vae(input_layer, enc_shapes, dec_shapes)\n",
    "train_op = tf.train.AdamOptimizer(rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least learning the model using batch gradient descent and then plotting the reconstruction.\n",
    "I also run a clustering experiment using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning: \n",
      "   - epoch:  0 161452.094421 153456.586868 7995.50754452\n",
      "   - epoch:  1 148867.538422 140123.548203 8743.99042225\n",
      "   - epoch:  2 146128.397415 137603.575699 8524.82198524\n",
      "   - epoch:  3 143870.160202 135592.97168 8277.18849659\n",
      "   - epoch:  4 142163.913437 133950.195892 8213.71784973\n",
      "Reconstruction: \n",
      " - reconstructing: Coat 0 0 0\n",
      " - reconstructing: T-shirt-top 0 0 1\n",
      " - reconstructing: T-shirt-top 0 0 2\n",
      " - reconstructing: Trouser 0 0 3\n",
      " - reconstructing: Coat 0 0 4\n",
      " - reconstructing: T-shirt-top 0 0 5\n",
      " - reconstructing: Coat 0 0 6\n",
      " - reconstructing: Sandal 0 0 7\n",
      " - reconstructing: Ankle boot 0 0 8\n",
      " - reconstructing: Coat 0 0 9\n",
      " - reconstructing: Dress 0 0 10\n",
      " - reconstructing: Shirt 0 0 11\n",
      " - reconstructing: Shirt 0 0 12\n",
      " - reconstructing: Shirt 0 0 13\n",
      " - reconstructing: Coat 0 0 14\n",
      " - reconstructing: Shirt 0 0 15\n",
      " - reconstructing: Sandal 0 0 16\n",
      " - reconstructing: Dress 0 0 17\n",
      " - reconstructing: Bag 0 0 18\n",
      " - reconstructing: Dress 0 0 19\n",
      " - reconstructing: Shirt 0 0 20\n",
      " - reconstructing: T-shirt-top 0 0 21\n",
      " - reconstructing: Trouser 0 0 22\n",
      " - reconstructing: Sneaker 0 0 23\n",
      " - reconstructing: Sandal 0 0 24\n",
      " - reconstructing: Dress 0 0 25\n",
      " - reconstructing: T-shirt-top 0 0 26\n",
      " - reconstructing: Sneaker 0 0 27\n",
      " - reconstructing: Bag 0 0 28\n",
      " - reconstructing: Sneaker 0 0 29\n",
      " - reconstructing: Dress 0 0 30\n",
      " - reconstructing: Ankle boot 0 0 31\n",
      " - reconstructing: Sandal 0 0 32\n",
      " - reconstructing: Trouser 0 0 33\n",
      " - reconstructing: T-shirt-top 0 0 34\n",
      " - reconstructing: Bag 0 0 35\n",
      " - reconstructing: Dress 0 0 36\n",
      " - reconstructing: Sandal 0 0 37\n",
      " - reconstructing: Trouser 0 0 38\n",
      " - reconstructing: Bag 0 0 39\n",
      " - reconstructing: Sandal 0 0 40\n",
      " - reconstructing: Coat 0 0 41\n",
      " - reconstructing: Ankle boot 0 0 42\n",
      " - reconstructing: Pullover 0 0 43\n",
      " - reconstructing: Bag 0 0 44\n",
      " - reconstructing: T-shirt-top 0 0 45\n",
      " - reconstructing: Trouser 0 0 46\n",
      " - reconstructing: Bag 0 0 47\n",
      " - reconstructing: Bag 0 0 48\n",
      " - reconstructing: Shirt 0 0 49\n",
      " - reconstructing: Coat 0 0 50\n",
      " - reconstructing: Bag 0 0 51\n",
      " - reconstructing: Bag 0 0 52\n",
      " - reconstructing: Trouser 0 0 53\n",
      " - reconstructing: Dress 0 0 54\n",
      " - reconstructing: Ankle boot 0 0 55\n",
      " - reconstructing: Dress 0 0 56\n",
      " - reconstructing: Ankle boot 0 0 57\n",
      " - reconstructing: Pullover 0 0 58\n",
      " - reconstructing: T-shirt-top 0 0 59\n",
      " - reconstructing: Pullover 0 0 60\n",
      " - reconstructing: Coat 0 0 61\n",
      " - reconstructing: Shirt 0 0 62\n",
      " - reconstructing: Sneaker 0 0 63\n",
      " - reconstructing: Ankle boot 0 0 64\n",
      " - reconstructing: Dress 0 0 65\n",
      " - reconstructing: Shirt 0 0 66\n",
      " - reconstructing: T-shirt-top 0 0 67\n",
      " - reconstructing: Shirt 0 0 68\n",
      " - reconstructing: Ankle boot 0 0 69\n",
      " - reconstructing: Sandal 0 0 70\n",
      " - reconstructing: Ankle boot 0 0 71\n",
      " - reconstructing: T-shirt-top 0 0 72\n",
      " - reconstructing: T-shirt-top 0 0 73\n",
      " - reconstructing: Sneaker 0 0 74\n",
      " - reconstructing: Trouser 0 0 75\n",
      " - reconstructing: Bag 0 0 76\n",
      " - reconstructing: Ankle boot 0 0 77\n",
      " - reconstructing: Pullover 0 0 78\n",
      " - reconstructing: Coat 0 0 79\n",
      " - reconstructing: Shirt 0 0 80\n",
      " - reconstructing: Shirt 0 0 81\n",
      " - reconstructing: Ankle boot 0 0 82\n",
      " - reconstructing: Dress 0 0 83\n",
      " - reconstructing: Coat 0 0 84\n",
      " - reconstructing: Pullover 0 0 85\n",
      " - reconstructing: Shirt 0 0 86\n",
      " - reconstructing: Sandal 0 0 87\n",
      " - reconstructing: Trouser 0 0 88\n",
      " - reconstructing: Ankle boot 0 0 89\n",
      " - reconstructing: Sneaker 0 0 90\n",
      " - reconstructing: Trouser 0 0 91\n",
      " - reconstructing: Ankle boot 0 0 92\n",
      " - reconstructing: Shirt 0 0 93\n",
      " - reconstructing: Sandal 0 0 94\n",
      " - reconstructing: Pullover 0 0 95\n",
      " - reconstructing: T-shirt-top 0 0 96\n",
      " - reconstructing: Coat 0 0 97\n",
      " - reconstructing: T-shirt-top 0 0 98\n",
      " - reconstructing: Dress 0 0 99\n",
      "Plotting\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # training\n",
    "    print(\"Learning: \")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        loss_lik   = 0.0\n",
    "        loss_div   = 0.0\n",
    "        for i in range(0, total_batch):\n",
    "            (batch, _) = data.train.next_batch(batch_size)\n",
    "            _, tot_loss, loss_likelihood, loss_divergence = sess.run(\n",
    "                (train_op, loss, neg_marginal_likelihood, KL_divergence),\n",
    "                feed_dict={ input_layer: batch }\n",
    "            )\n",
    "            total_loss += tot_loss\n",
    "            loss_lik   += loss_likelihood\n",
    "            loss_div   += loss_divergence\n",
    "        print(\"   - epoch: \", epoch, total_loss, loss_lik, loss_div)\n",
    "\n",
    "    \n",
    "    #  creating reconstruction from test images\n",
    "    print(\"Reconstruction: \")\n",
    "    (images, labels) = data.test.next_batch(100)\n",
    "    a = 0\n",
    "    b = 0\n",
    "    latent_img = []\n",
    "    for i in range(0, 100):\n",
    "        print(\" - reconstructing: \" + labels_dict[labels[i]], a, b, i)\n",
    "        y_out, u    = sess.run((y, z), feed_dict={input_layer: images[i].reshape(1, 784)})\n",
    "        y_img       = y_out.reshape(28, 28)\n",
    "        latent_img += [(y_img, labels_dict[labels[i]])]\n",
    "    latent_img = sorted(latent_img, key = lambda x : x[1])\n",
    "\n",
    "    # plotting\n",
    "    print(\"Plotting\")\n",
    "    f, axarr = plt.subplots(10, 10)\n",
    "    for i in range(0, 100):        \n",
    "        if a == 10:\n",
    "            a = 0\n",
    "            b += 1\n",
    "\n",
    "        axarr[a, b].set_title(latent_img[i][1])\n",
    "        axarr[a, b].imshow(latent_img[i][0], cmap=plt.get_cmap('gray'))  \n",
    "        a += 1\n",
    "        \n",
    "    f.subplots_adjust(hspace = 0.7)\n",
    "    f.set_size_inches(25.0, 25.0, forward=True)\n",
    "    plt.savefig(\"result/prediction.png\")        \n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
